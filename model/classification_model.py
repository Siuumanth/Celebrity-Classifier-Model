# -*- coding: utf-8 -*-
"""classification_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S7wvSNWAwrkhU2le-gjNQT94mGGQQZ3s
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline



face_cascade = cv2.CascadeClassifier('D:/code/fullstack/Celeb classifier sumanth/model/opencv/haarcascades/haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('D:/code/fullstack/Celeb classifier sumanth/model/opencv/haarcascades/haarcascade_eye.xml')



# @title Default title text
#stackoverflow code
import numpy as np
import pywt               # importing wavelet
import cv2

def w2d(img, mode='haar', level=1):
    imArray = img
    #Datatype conversions
    #convert to grayscale
    imArray = cv2.cvtColor( imArray,cv2.COLOR_RGB2GRAY )
    #convert to float
    imArray =  np.float32(imArray)
    imArray /= 255
    # compute coefficients
    coeffs=pywt.wavedec2(imArray, mode, level=level)

    #Process Coefficients
    coeffs_H=list(coeffs)
    coeffs_H[0] *= 0

    # reconstruction
    imArray_H=pywt.waverec2(coeffs_H, mode)
    imArray_H *= 255
    imArray_H =  np.uint8(imArray_H)

    return imArray_H

import os
import shutil
def get_cropped_image_if_2_eyes(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    for (x,y,w,h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        roi_color = img[y:y+h, x:x+w]
        eyes = eye_cascade.detectMultiScale(roi_gray)
        if len(eyes) >= 2:
            return roi_color.astype('uint8')

path_cropped='D:/code/fullstack/Celeb classifier sumanth/model/DATASET/cropped'
path_data='D:/code/fullstack/Celeb classifier sumanth/model/DATASET'

import os
img_dirs = []
for entry in os.scandir(path_data):
    if entry.is_dir():
        img_dirs.append(entry.path)



cropped_image_dirs = []
celebrity_file_names_dict = {}
for img_dir in img_dirs:
    celebrity_name = img_dir.split('\\')[-1]
    if celebrity_name=='cropped':
        continue
    count = 1
    
    celebrity_file_names_dict[celebrity_name] = []
    for entry in os.scandir(img_dir):       
        roi_color = get_cropped_image_if_2_eyes(entry.path)
        if roi_color is not None:
            cropped_folder = path_cropped + '/cropped' + celebrity_name
            if not os.path.exists(cropped_folder):

                cropped_image_dirs.append(cropped_folder)

            cropped_file_name = celebrity_name + str(count) + ".png"
            cropped_file_path = cropped_folder + "/" + cropped_file_name
            celebrity_file_names_dict[celebrity_name].append(cropped_file_path)
            count += 1

            



class_dict = {}
count = 0
for celebrity_name in celebrity_file_names_dict.keys():
    class_dict[celebrity_name] = count
    count = count + 1



X=[]
y=[]
for celebrity_names,training_files in celebrity_file_names_dict.items():  #key,list
  for training_img in training_files:
    if not os.path.exists(training_img):
       continue
    img=cv2.imread(training_img)
    if img is None:
      continue
    scaled_raw_img=cv2.resize(img,(32,32))
    img_har = w2d(img,'db1',5)
    scaled_img_har = cv2.resize(img_har, (32, 32))
    combined_img = np.vstack((scaled_raw_img.reshape(32*32*3,1),scaled_img_har.reshape(32*32,1))) #we stack wlet img and real image on top of eachother
    X.append(combined_img)
    y.append(class_dict[celebrity_names])




#**NOW WE TRAIN THE MODELL**








from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=0.25)

X_train= np.array(X_train).reshape((125, 4096))
X_train=X_train.tolist()


X_test= np.array(X_test).reshape((42, 4096))

pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel = 'rbf', C = 1))])
pipe.fit(X_train, y_train)


from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV

model_params = {
    'svm': {
        'model': svm.SVC(gamma='auto',probability=True),
        'params' : {
            'svc__C': [1,10,100,1000],
            'svc__kernel': ['rbf','linear','poly']
        }
    },
    'random_forest': {
        'model': RandomForestClassifier(),
        'params' : {
            'randomforestclassifier__n_estimators': [1,5,10]
        }
    },
    'logistic_regression' : {
        'model': LogisticRegression(solver='liblinear',multi_class='auto'),
        'params': {
            'logisticregression__C': [1,5,10]
        }
    }
}


scores = []
best_estimators = {}
import pandas as pd
for algo, mp in model_params.items():
    pipe = make_pipeline(StandardScaler(), mp['model'])
    clf =  GridSearchCV(pipe, mp['params'], cv=5, return_train_score=False)
    clf.fit(X_train, y_train)
    scores.append({
        'model': algo,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })
    best_estimators[algo] = clf.best_estimator_

df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
best_clf = best_estimators['svm']




import joblib
from joblib import dump

dump(best_clf,'D:/code/fullstack/Celeb classifier sumanth/server/artifacts 2/saved2.joblib')
print("FILE SUCCESSFULLY CREATED again ig")



'''
# Save the model as a pickle in a file
joblib.dump(pipe, 'D:/code/fullstack/Celeb classifier sumanth/server/artifacts/saved_model_NEW.pkl')




import json
with open("D:/code/fullstack/Celeb classifier sumanth/server/artifacts/class_dictionary.json","w") as f:
    f.write(json.dumps(class_dict))

'''





    
    
def celeb_number_to_name(n):
   for k,v in class_dict:
      if k==n:
         return v   
    
    



def find_celeb(image_path):
    imagee=cv2.imread(image_path)
    cropped_imagee = get_cropped_image_if_2_eyes(imagee)
    if cropped_imagee is not None:
       # Resize the cropped image to 32x32
       scaled_imagee = cv2.resize(cropped_imagee, (32, 32))
       print("Image cropped and resized successfully.")
    else:
       print("Failed to crop image.")

    wl_imagee = w2d(cropped_imagee,'db1',5)
    scaled_wl_imagee = cv2.resize(wl_imagee, (32, 32))
    final_imagee= np.vstack((scaled_imagee.reshape(32*32*3,1),scaled_wl_imagee.reshape(32*32,1)))
    final_imagee=final_imagee.reshape(1,4096)
    predictions = best_clf.predict(final_imagee)

    print(celeb_number_to_name(predictions))





